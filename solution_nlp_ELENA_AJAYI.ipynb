{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elenaajayi/Computer-Vision-Fundamentals---CUS-754/blob/main/solution_nlp_ELENA_AJAYI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 1: Data Loading and Exploring the Data"
      ],
      "metadata": {
        "id": "QRrSjCjGRzri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "SPSeFa9PQiV2",
        "outputId": "e58ce277-a96a-43e5-eb96-72a244f09c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "ERROR:root:Error loading risk_factors.csv: [Errno 2] No such file or directory: 'risk_factors.csv'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'risk_factors.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ce033ea70c06>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Load Risk Factors dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mrisk_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'risk_factors.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mvalidate_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrisk_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'english_keywords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'keywords_arabic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Risk Factors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Risk Factors Data loaded successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'risk_factors.csv'"
          ]
        }
      ],
      "source": [
        "#Importing all necessary libararies\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Setup logging to provide robust error handling and info messages\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "logging.info(\"Starting Module 1: Data Ingestion & Initial Exploration\")\n",
        "\n",
        "def validate_columns(df, required_columns, df_name=\"DataFrame\"):\n",
        "    \"\"\"\n",
        "    Validate that the DataFrame contains all required columns.\n",
        "    \"\"\"\n",
        "    missing = [col for col in required_columns if col not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{df_name} is missing required columns: {missing}\")\n",
        "    else:\n",
        "        logging.info(f\"{df_name} contains all required columns: {required_columns}\")\n",
        "\n",
        "# Load Risk Factors dataset\n",
        "try:\n",
        "    risk_factors = pd.read_csv('risk_factors.csv')\n",
        "    validate_columns(risk_factors, ['english_keywords', 'keywords_arabic'], 'Risk Factors')\n",
        "    logging.info(\"Risk Factors Data loaded successfully\")\n",
        "    logging.info(risk_factors.head())\n",
        "except Exception as e:\n",
        "    logging.error(\"Error loading risk_factors.csv: %s\", e)\n",
        "    raise\n",
        "\n",
        "# Load Thematic Mapping dataset\n",
        "try:\n",
        "    thematic_mapping = pd.read_csv('thematic_mapping.csv')\n",
        "    validate_columns(thematic_mapping, ['risk_factor', 'cluster'], 'Thematic Mapping')\n",
        "    logging.info(\"Thematic Mapping loaded successfully\")\n",
        "    logging.info(thematic_mapping.head())\n",
        "except Exception as e:\n",
        "    logging.error(\"Error loading thematic_mapping.csv: %s\", e)\n",
        "    raise\n",
        "\n",
        "# Load English news articles\n",
        "try:\n",
        "    news_articles_eng = pd.read_csv('news-articles-eng.csv')\n",
        "    validate_columns(news_articles_eng, ['content', 'date', 'location_key'], 'English News Articles')\n",
        "    logging.info(\"English News Articles loaded successfully\")\n",
        "    logging.info(news_articles_eng.head())\n",
        "except Exception as e:\n",
        "    logging.error(\"Error loading news-articles-eng.csv: %s\", e)\n",
        "    raise\n",
        "\n",
        "# Load Arabic news articles\n",
        "try:\n",
        "    news_articles_ara = pd.read_csv('news-articles-ara.csv')\n",
        "    validate_columns(news_articles_ara, ['content', 'date', 'location_key'], 'Arabic News Articles')\n",
        "    logging.info(\"Arabic News Articles loaded successfully\")\n",
        "    logging.info(news_articles_ara.head())\n",
        "except Exception as e:\n",
        "    logging.error(\"Error loading news-articles-ara.csv: %s\", e)\n",
        "    raise\n",
        "\n",
        "# Load Geographic Taxonomy files (assumed to be in pickle format)\n",
        "try:\n",
        "    with open('id_english_location_name.pkl', 'rb') as f:\n",
        "        geo_english = pickle.load(f)\n",
        "    logging.info(\"Geographic Taxonomy (English) loaded successfully\")\n",
        "    logging.info(geo_english)\n",
        "except Exception as e:\n",
        "    logging.error(\"Error loading id_english_location_name.pkl: %s\", e)\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    with open('id_arabic_location_name.pkl', 'rb') as f:\n",
        "        geo_arabic = pickle.load(f)\n",
        "    logging.info(\"Geographic Taxonomy (Arabic) loaded successfully\")\n",
        "    logging.info(geo_arabic)\n",
        "except Exception as e:\n",
        "    logging.error(\"Error loading id_arabic_location_name.pkl: %s\", e)\n",
        "    raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 2"
      ],
      "metadata": {
        "id": "qKUNytn5SQp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logging.info(\"Starting Module 2: Data Cleaning & Preprocessing\")\n",
        "\n",
        "def clean_text(text, language='english'):\n",
        "    \"\"\"\n",
        "    Clean text by converting to lowercase, removing punctuation,\n",
        "    tokenizing, and removing stopwords.\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    if language == 'english':\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    else:\n",
        "        # Enhanced Arabic stopword list; this is a sample list.\n",
        "        arabic_stopwords = {\"في\", \"على\", \"من\", \"ما\", \"مع\", \"لا\", \"إلى\", \"عن\", \"أن\", \"هذا\",\n",
        "                            \"و\", \"إلا\", \"لكن\", \"ذلك\", \"هذه\", \"هو\", \"هي\", \"هناك\", \"أو\",\n",
        "                            \"إما\", \"لم\", \"لن\", \"قد\", \"بعد\", \"كما\"}\n",
        "        stop_words = arabic_stopwords\n",
        "\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Clean the 'english_keywords' in risk factors\n",
        "risk_factors['clean_english_keywords'] = risk_factors['english_keywords'].apply(lambda x: clean_text(x, language='english'))\n",
        "logging.info(\"Cleaned Risk Factors (English Keywords):\")\n",
        "logging.info(risk_factors[['english_keywords', 'clean_english_keywords']].head())\n",
        "\n",
        "# Improved translation: Use googletrans if available; else, use dummy translation.\n",
        "try:\n",
        "    from googletrans import Translator\n",
        "    translator = Translator()\n",
        "\n",
        "    def translate_to_arabic(text):\n",
        "        \"\"\"\n",
        "        Translate English text to Arabic using googletrans.\n",
        "        If translation fails, fall back to dummy translation.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            translated = translator.translate(text, dest='ar')\n",
        "            return translated.text\n",
        "        except Exception as e:\n",
        "            logging.error(\"Translation error for text '%s': %s\", text, e)\n",
        "            return text + '_ara'\n",
        "except ImportError:\n",
        "    logging.warning(\"googletrans library not found. Falling back to dummy translation.\")\n",
        "    def translate_to_arabic(text):\n",
        "        return text + '_ara'\n",
        "\n",
        "# Populate the Arabic keywords column if it is empty\n",
        "risk_factors['keywords_arabic'] = risk_factors.apply(\n",
        "    lambda row: translate_to_arabic(row['english_keywords'])\n",
        "                if pd.isna(row['keywords_arabic']) or row['keywords_arabic'].strip() == ''\n",
        "                else row['keywords_arabic'],\n",
        "    axis=1\n",
        ")\n",
        "logging.info(\"Risk Factors with Arabic Keywords (After Translation):\")\n",
        "logging.info(risk_factors[['english_keywords', 'keywords_arabic']].head())\n",
        "\n",
        "# Preprocess news articles content\n",
        "if 'content' in news_articles_eng.columns:\n",
        "    news_articles_eng['clean_content'] = news_articles_eng['content'].apply(lambda x: clean_text(x, language='english'))\n",
        "    logging.info(\"Sample Cleaned Content for English News Articles:\")\n",
        "    logging.info(news_articles_eng[['content', 'clean_content']].head())\n",
        "\n",
        "if 'content' in news_articles_ara.columns:\n",
        "    news_articles_ara['clean_content'] = news_articles_ara['content'].apply(lambda x: clean_text(x, language='arabic'))\n",
        "    logging.info(\"Sample Cleaned Content for Arabic News Articles:\")\n",
        "    logging.info(news_articles_ara[['content', 'clean_content']].head())\n",
        "\n",
        "def map_location(key, geo_dict):\n",
        "    \"\"\"\n",
        "    Map a geographic key to its corresponding location name.\n",
        "    \"\"\"\n",
        "    return geo_dict.get(key, 'Unknown')\n",
        "\n",
        "if 'location_key' in news_articles_eng.columns:\n",
        "    news_articles_eng['location_name'] = news_articles_eng['location_key'].apply(lambda x: map_location(x, geo_english))\n",
        "    logging.info(\"English News Articles with Mapped Location Names:\")\n",
        "    logging.info(news_articles_eng[['location_key', 'location_name']].head())\n",
        "\n",
        "if 'location_key' in news_articles_ara.columns:\n",
        "    news_articles_ara['location_name'] = news_articles_ara['location_key'].apply(lambda x: map_location(x, geo_arabic))\n",
        "    logging.info(\"Arabic News Articles with Mapped Location Names:\")\n",
        "    logging.info(news_articles_ara[['location_key', 'location_name']].head())er steps.\")\n"
      ],
      "metadata": {
        "id": "b4-TO5kNSTC_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}